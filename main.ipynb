{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5dae3e9",
   "metadata": {},
   "source": [
    "### Phi3-4K-mini 모델불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2cd764d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 27.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry to hear that your dog has been vomiting. Vomiting can be caused by a variety of factors, including dietary indiscretion (eating something they shouldn't have), infections, parasites, or more serious conditions like pancreatitis or gastrointestinal obstruction. It's important to monitor your dog closely. If the vomiting persists, is accompanied by other symptoms like diarrhea, lethargy, or loss of appetite, or if your dog seems to be in distress, it's crucial to seek veterinary care immediately. In the meantime, ensure your dog has access to fresh water and try to keep them calm and comfortable.<|end|>\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\", torch_dtype = \"auto\",  trust_remote_code=False)\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"My dog vomitted since yesterday with no reason\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "\tmessages,\n",
    "\tadd_generation_prompt=True,\n",
    "\ttokenize=True,\n",
    "\treturn_dict=True,\n",
    "\treturn_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=416)\n",
    "print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0fc474",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U \"transformers>=4.45\" accelerate safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07dfaf45",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'trl'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtrl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SFTTrainer\n\u001b[32m      7\u001b[39m BASE = \u001b[33m\"\u001b[39m\u001b[33mmicrosoft/Phi-3-mini-4k-instruct\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m tokenizer = AutoTokenizer.from_pretrained(BASE)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'trl'"
     ]
    }
   ],
   "source": [
    "# pip install -U transformers datasets peft accelerate bitsandbytes trl\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "\n",
    "BASE = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE)\n",
    "\n",
    "# (옵션) 4비트 로드로 QLoRA\n",
    "load_kwargs = dict(\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    load_in_4bit=True, # QLoRA면 True 그냥LoRA면 False\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(BASE, **load_kwargs)\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora = LoraConfig(\n",
    "    r=16, lora_alpha=32, target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora)\n",
    "\n",
    "# 예시용 데이터셋: {\"messages\":[{\"role\":\"user\",\"content\":\"...\"},{\"role\":\"assistant\",\"content\":\"...\"}]}\n",
    "ds = load_dataset(\"json\", data_files={\"train\":\"train.jsonl\", \"eval\":\"eval.jsonl\"})\n",
    "\n",
    "def format_example(ex):\n",
    "    # Phi-3 chat 템플릿 활용해서 supervised target 만들기\n",
    "    msgs = ex[\"messages\"]\n",
    "    text = tokenizer.apply_chat_template(msgs, add_generation_prompt=False, tokenize=False)\n",
    "    return {\"text\": text}\n",
    "\n",
    "ds = ds.map(format_example, remove_columns=ds[\"train\"].column_names)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"phi3_lora_out\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=20,\n",
    "    save_steps=200,\n",
    "    eval_strategy=\"steps\",\n",
    "    fp16=True,                               # Ampere↑에서 bf16도 가능\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"eval\"],\n",
    "    args=args,\n",
    "    dataset_text_field=\"text\",\n",
    "    packing=True,                            # 여러 샘플을 한 시퀀스로 패킹(효율↑)\n",
    "    max_seq_length=2048\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "model.save_pretrained(\"phi3_lora_out/adapter\")\n",
    "tokenizer.save_pretrained(\"phi3_lora_out/adapter\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1016736a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting trl\n",
      "  Using cached trl-0.23.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: accelerate>=1.4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from trl) (1.10.1)\n",
      "Requirement already satisfied: datasets>=3.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from trl) (3.4.1)\n",
      "Collecting transformers>=4.56.1 (from trl)\n",
      "  Using cached transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from accelerate>=1.4.0->trl) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from accelerate>=1.4.0->trl) (24.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\user\\anaconda3\\lib\\site-packages (from accelerate>=1.4.0->trl) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\user\\anaconda3\\lib\\site-packages (from accelerate>=1.4.0->trl) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from accelerate>=1.4.0->trl) (2.6.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from accelerate>=1.4.0->trl) (0.29.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from accelerate>=1.4.0->trl) (0.5.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets>=3.0.0->trl) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets>=3.0.0->trl) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets>=3.0.0->trl) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets>=3.0.0->trl) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets>=3.0.0->trl) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets>=3.0.0->trl) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets>=3.0.0->trl) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=3.0.0->trl) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets>=3.0.0->trl) (3.10.11)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=3.0.0->trl) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=3.0.0->trl) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=3.0.0->trl) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=3.0.0->trl) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=3.0.0->trl) (6.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=3.0.0->trl) (1.18.3)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets>=3.0.0->trl) (3.10)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets>=3.0.0->trl) (0.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2025.1.31)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.1.6)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate>=1.4.0->trl) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from tqdm>=4.66.3->datasets>=3.0.0->trl) (0.4.6)\n",
      "Collecting huggingface_hub>=0.21.0 (from accelerate>=1.4.0->trl)\n",
      "  Using cached huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers>=4.56.1->trl) (2024.11.6)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers>=4.56.1->trl)\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate>=1.4.0->trl) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas->datasets>=3.0.0->trl) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas->datasets>=3.0.0->trl) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n",
      "Using cached trl-0.23.1-py3-none-any.whl (564 kB)\n",
      "Using cached transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "Using cached huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n",
      "Using cached tokenizers-0.22.1-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "Installing collected packages: huggingface_hub, tokenizers, transformers, trl\n",
      "\n",
      "  Attempting uninstall: huggingface_hub\n",
      "\n",
      "    Found existing installation: huggingface-hub 0.29.3\n",
      "\n",
      "    Uninstalling huggingface-hub-0.29.3:\n",
      "\n",
      "      Successfully uninstalled huggingface-hub-0.29.3\n",
      "\n",
      "   ---------------------------------------- 0/4 [huggingface_hub]\n",
      "   ---------------------------------------- 0/4 [huggingface_hub]\n",
      "   ---------------------------------------- 0/4 [huggingface_hub]\n",
      "   ---------------------------------------- 0/4 [huggingface_hub]\n",
      "  Attempting uninstall: tokenizers\n",
      "   ---------------------------------------- 0/4 [huggingface_hub]\n",
      "    Found existing installation: tokenizers 0.21.1\n",
      "   ---------------------------------------- 0/4 [huggingface_hub]\n",
      "    Uninstalling tokenizers-0.21.1:\n",
      "   ---------------------------------------- 0/4 [huggingface_hub]\n",
      "      Successfully uninstalled tokenizers-0.21.1\n",
      "   ---------------------------------------- 0/4 [huggingface_hub]\n",
      "   ---------- ----------------------------- 1/4 [tokenizers]\n",
      "   ---------- ----------------------------- 1/4 [tokenizers]\n",
      "   ---------- ----------------------------- 1/4 [tokenizers]\n",
      "   ---------- ----------------------------- 1/4 [tokenizers]\n",
      "   ---------- ----------------------------- 1/4 [tokenizers]\n",
      "   ---------- ----------------------------- 1/4 [tokenizers]\n",
      "   ---------- ----------------------------- 1/4 [tokenizers]\n",
      "   ---------- ----------------------------- 1/4 [tokenizers]\n",
      "   ---------- ----------------------------- 1/4 [tokenizers]\n",
      "   ---------- ----------------------------- 1/4 [tokenizers]\n",
      "   ---------- ----------------------------- 1/4 [tokenizers]\n",
      "   ---------- ----------------------------- 1/4 [tokenizers]\n",
      "   ---------- ----------------------------- 1/4 [tokenizers]\n",
      "   ---------- ----------------------------- 1/4 [tokenizers]\n",
      "   ---------- ----------------------------- 1/4 [tokenizers]\n",
      "   ---------- ----------------------------- 1/4 [tokenizers]\n",
      "   ---------- ----------------------------- 1/4 [tokenizers]\n",
      "   ---------- ----------------------------- 1/4 [tokenizers]\n",
      "  Attempting uninstall: transformers\n",
      "   ---------- ----------------------------- 1/4 [tokenizers]\n",
      "    Found existing installation: transformers 4.49.0\n",
      "   ---------- ----------------------------- 1/4 [tokenizers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "    Uninstalling transformers-4.49.0:\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "      Successfully uninstalled transformers-4.49.0\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   -------------------- ------------------- 2/4 [transformers]\n",
      "   ------------------------------ --------- 3/4 [trl]\n",
      "   ------------------------------ --------- 3/4 [trl]\n",
      "   ------------------------------ --------- 3/4 [trl]\n",
      "   ------------------------------ --------- 3/4 [trl]\n",
      "   ---------------------------------------- 4/4 [trl]\n",
      "\n",
      "Successfully installed huggingface_hub-0.35.3 tokenizers-0.22.1 transformers-4.57.1 trl-0.23.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\User\\anaconda3\\Lib\\site-packages\\~okenizers'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "%pip install trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429b1026",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
